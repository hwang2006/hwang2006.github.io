---
title: "A Deep Dive into Distributed Training of Large-scale Language Modeling with PyTorch on a Supercomputer"
collection: talks
type: "Tutorial"
venue: "2024 한국소프트웨어종합학술대회(KSC2024)
date: 2024-12-18
location: "여수엑스포컨벤션센터"
---

This tutorial presents a comprehensive, in-depth guide for large-scale distributed training of LLMs on supercomputers managed with SLURM. It briefly covers some basics of collective communications in message passing including gather, scatter and all-gather operations, delving into data parallelism techniques such as Data Parallelism (DP) and Distributed Data Parallelism (DDP) in PyTorch, and model parallelism techniques including Tensor Parallelism, Pipeline Parallelism, and 3D Parallelism, with hands-on PyTorch code examples. It also covers how to set up and leverage distributed training tools like NVIDIA Megatron-LM and Microsoft DeepSpeed to efficiently run the PyTorch codes using multiple GPUs on a supercomputer.


[KSC2024 Tutorial Link](https://www.kiise.or.kr/conference/main/getContent.do?CC=ksc&CS=2024&PARENT_ID=011100&content_no=2141)

