---
tititle: "Best Practices for Generative AI with LLMs on a Supercomputer"
collection: talks
type: "Tutorial"
venue: "2024 한국컴퓨터종합학술대회 (KCC2024)"
date: 2024-06-26
location: "ICC 제주"
---

Generative AI with LLMs refers to the use of large language models like GPT-3 for generating human-like content, spanning text, images and even code. LLMs are trained on a vast amount of data and code, and usually carefully prompt-engineered or fine-tuned to suit specific downstream tasks such as Chatbots, Translation, Question Answering and Summarization. The contents and python codes of this tutorial are originated from the 16-hour “Generative AI with LLMs” course offered by the DeepLearning.AI. This tutorial will mainly cover the key concepts and practices of a typical LLM-powered Generative AI lifecycle, from data gathering and model selection, to instruction fine-tuning and RLHF-based alignment to human preference, to performance evaluation and deployment. For hands-on exercises, students will have access to the KISTI GPU cluster known as Neuron, which consists of 65 nodes with 260 GPUs (120 of NVIDIA A100 and 140 of NVIDIA V100 GPUs) running SLURM as for its workload manager.

[KCC2024 Tutorial Link](https://www.kiise.or.kr/conference/main/getContent.do?CC=KCC&CS=2024&PARENT_ID=011500&content_no=2011)
